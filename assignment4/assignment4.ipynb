{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landon Buell\n",
    "Marek Petrik\n",
    "CS 750.01\n",
    "18 Feb 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [25%] \n",
    "### In this exercise, we will predict the number of applications received using the other variables in the College (ISLR::College) data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Private  Apps  Accept  Enroll  Top10perc  \\\n",
      "Abilene Christian University        1  1660    1232     721         23   \n",
      "Adelphi University                  1  2186    1924     512         16   \n",
      "Adrian College                      1  1428    1097     336         22   \n",
      "Agnes Scott College                 1   417     349     137         60   \n",
      "Alaska Pacific University           1   193     146      55         16   \n",
      "\n",
      "                              Top25perc  F.Undergrad  P.Undergrad  Outstate  \\\n",
      "Abilene Christian University         52         2885          537      7440   \n",
      "Adelphi University                   29         2683         1227     12280   \n",
      "Adrian College                       50         1036           99     11250   \n",
      "Agnes Scott College                  89          510           63     12960   \n",
      "Alaska Pacific University            44          249          869      7560   \n",
      "\n",
      "                              Room.Board  Books  Personal  PhD  Terminal  \\\n",
      "Abilene Christian University        3300    450      2200   70        78   \n",
      "Adelphi University                  6450    750      1500   29        30   \n",
      "Adrian College                      3750    400      1165   53        66   \n",
      "Agnes Scott College                 5450    450       875   92        97   \n",
      "Alaska Pacific University           4120    800      1500   76        72   \n",
      "\n",
      "                              S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
      "Abilene Christian University       18.1           12    7041         60  \n",
      "Adelphi University                 12.2           16   10527         56  \n",
      "Adrian College                     12.9           30    8735         54  \n",
      "Agnes Scott College                 7.7           37   19016         59  \n",
      "Alaska Pacific University          11.9            2   10922         15  \n",
      "(777, 16) (777,)\n"
     ]
    }
   ],
   "source": [
    "# Load in data set, print out head\n",
    "college = pd.read_csv('college.csv',index_col=0)\n",
    "college['Private'] = college['Private'].map({'Yes':1,'No':0})\n",
    "print(college.head())\n",
    "\n",
    "# create X matrix & target vector\n",
    "X = college.drop(['Apps','Private'],axis=1)\n",
    "y = college['Apps']\n",
    "X.to_numpy()\n",
    "y.to_numpy()\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fit a linear model using least squares on the training set, and report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error determined by MSE: 1185095.6869691072\n",
      "This seems unreasonably large!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit least sq. model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = linreg.predict(xtest)\n",
    "linreg_MSE = metrics.mean_squared_error(ytest,ypred)\n",
    "print(\"Testing Error determined by MSE:\",linreg_MSE)\n",
    "print(\"This seems unreasonably large!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use best subset selection with cross-validation. Report the test error obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doane College</th>\n",
       "      <td>709.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smith College</th>\n",
       "      <td>1598.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>2479.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spring Arbor College</th>\n",
       "      <td>362.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>St. Mary's College of California</th>\n",
       "      <td>1611.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUNY College at Plattsburgh</th>\n",
       "      <td>3583.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accept  Enroll  F.Undergrad  P.Undergrad\n",
       "Doane College                      709.0   244.0       1022.0        411.0\n",
       "Smith College                     1598.0   632.0       2479.0         95.0\n",
       "Spring Arbor College               362.0   181.0       1501.0        353.0\n",
       "St. Mary's College of California  1611.0   465.0       2615.0        248.0\n",
       "SUNY College at Plattsburgh       3583.0   853.0       5004.0        475.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest , f_regression\n",
    "# I wasn't sure how to do this with X-val in python b/c of the arguments required!\n",
    "\n",
    "Kbest = SelectKBest(score_func=f_regression,k=4)\n",
    "Kbest.fit(xtrain,ytrain)\n",
    "\n",
    "# create new training data set\n",
    "xtrain_new_tp = xtrain.transpose()[Kbest.get_support()]\n",
    "xtrain_new = xtrain_new_tp.transpose()\n",
    "xtrain_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error determined by MSE: 1756389.6292415215\n",
      "It's increased! What have I done wrong???\n"
     ]
    }
   ],
   "source": [
    "# Create new linear regression class\n",
    "linreg2 = LinearRegression()\n",
    "linreg2.fit(xtrain_new,ytrain)\n",
    "\n",
    "# new testing data set\n",
    "xtest_new_tp = xtest.transpose()[Kbest.get_support()]\n",
    "xtest_new = xtest_new_tp.transpose()\n",
    "\n",
    "# new prediction on data subset\n",
    "ypred2 = linreg2.predict(xtest_new)\n",
    "linreg2_MSE = metrics.mean_squared_error(ytest,ypred2)\n",
    "print(\"Testing Error determined by MSE:\",linreg2_MSE)\n",
    "print(\"It's increased! What have I done wrong???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fit a ridge regression model on the training set, with λ chosen by cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit a lasso model on the training set, with λ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coeﬃcient estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Brieﬂy comment on the results obtained. How accurately can we predict the number of college applications received? Is there much diﬀerence among the test errors resulting from these approaches? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [25%] \n",
    "### We will try to predict per capita crime rate in the Boston dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load in Data set & make frame\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston['data'],columns=boston['feature_names'])\n",
    "\n",
    "# make intor X & y objects\n",
    "y = X['CRIM']\n",
    "X = X.drop(['CRIM'],axis=1)\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Try out best subset selection, the lasso, ridge regression, and PCR on this problem. Present and discuss results for the approaches that you consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Kbest = SelectKBest(score_func=f_regression,k=4)\n",
    "Kbest.fit(xtrain,ytrain)\n",
    "\n",
    "xtrain_new_tp = xtrain.transpose()[Kbest.get_support()]\n",
    "xtrain_new = xtrain_new_tp.transpose()\n",
    "xtest_new_tp = xtest.transpose()[Kbest.get_support()]\n",
    "xtest_new = xtest_new_tp.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error determined by MSE: 47.26504990228277\n",
      "Testing Error determined by MSE: 47.17110186098528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# train Lasso Instance on initial dataset\n",
    "lasso_1 = Lasso()\n",
    "lasso_1.fit(xtrain,ytrain)\n",
    "ypred_1 = lasso_1.predict(xtest)\n",
    "print(\"Testing Error determined by MSE:\",metrics.mean_squared_error(ytest,ypred_1))\n",
    "\n",
    "# train Lasso Instance on Kbest dataset\n",
    "lasso_2 = Lasso()\n",
    "lasso_2.fit(xtrain_new,ytrain)\n",
    "ypred_2 = lasso_2.predict(xtest_new)\n",
    "print(\"Testing Error determined by MSE:\",metrics.mean_squared_error(ytest,ypred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error determined by MSE: 46.26923997391517\n",
      "Testing Error determined by MSE: 47.17110186098528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train Ridge Instance on initial dataset\n",
    "ridge_1 = Ridge()\n",
    "ridge_1.fit(xtrain,ytrain)\n",
    "ypred_1 = ridge_1.predict(xtest)\n",
    "print(\"Testing Error determined by MSE:\",metrics.mean_squared_error(ytest,ypred_1))\n",
    "\n",
    "# train Ridge Instance on Kbest dataset\n",
    "ridge_2 = Lasso()\n",
    "ridge_2.fit(xtrain_new,ytrain)\n",
    "ypred_2 = ridge_2.predict(xtest_new)\n",
    "print(\"Testing Error determined by MSE:\",metrics.mean_squared_error(ytest,ypred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error. \n",
    "\n",
    "Both Ridge and Lasso seem to work quite well on this model. Unforuately, The K best features from the data set for K = 4 seems to offer little, or occassionally no improvement from just the base data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [25%] \n",
    "### Suppose we have a linear regression problem with P features. We estimate the coeﬃcients in the linear regression model by minimizing the RSS for the ﬁrst p features:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\Bigg( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_j x_{ij} \\Bigg)^2  $$\n",
    "\n",
    "### Where $p \\leq P$ for parts (1) through (5) , indicate which is correct.  Brieﬂy justify your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. As we increase p from 1 to P, the training RSS will typically:\n",
    "As we add more features, our training error go down. For a short while, It will likely produce a better fit for the data and perform well on the testing or validation set. Thus with each sucuessive $p$ added, the difference between each $y_i$ and $\\hat{y}_i$ decreases until we run into the problem of overfitting the data set. Thus, the RSS will (v.) Decrease initially, and then eventually start increasing in a U shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. As we increase p from 1 to P, the training MSE will typically:\n",
    "Just like RSS, adding more features decreases the training error, at a certain point, adding more and more features will cause the training set to be overfitted and thus perform poorly on the testing or validation set. Again, the MSE will (v.) Decrease initially, and then eventually start increasing in a U shape.(ISL, fig. 2.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. As we increase p from 1 to P, the training squared bias will typically:\n",
    "\n",
    "The squared bais is the expectation value of the difference between the predicted and true output of a model. With each sucessive feature, the model is prone of overfitting and thus performs worse and worse on the testing set. This when paired with the fact that we are squaring the result, always produces a sucessivly larger positive number. Thus, the squared bias will (iii.) Steadily decrease.(ISL, fig. 2.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. As we increase p from 1 to P, the training variance will typically:\n",
    "\n",
    "Adding more features will generally cause the variance to (ii.) steadily increase (ISL, fig. 2.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. As we increase p from 1 to P, the irreducible error (Bayes error) will typically:\n",
    "\n",
    "Adding more features will cause the Bayes Error to reduce initially,, because the irreducible error always seeks the minimum possible value. However, when overfitting is comes into play, we can then say that the error will (v.) Decrease initially, and then eventually start increasing in a U shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [25%] \n",
    "### Suppose we estimate the regression coeﬃcients in a linear regression model by minimizing:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\Bigg( y_i - \\beta_0 - \\sum_{j=1}^{p}\\beta_{j}x_{ij} \\Bigg)^2 $$\n",
    "\n",
    "#### Subject to:\n",
    "\n",
    "$$ \\sum_{j=1}^{p} |\\beta_j|^2 \\geq s $$\n",
    "\n",
    "### for a particular value of s. For parts (1) through (5), indicate which of i. through v. is correct. Justify your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. As we increase s from 0, the training RSS will typically: \n",
    "\n",
    "Increasing $s$ to a sufficiently large value, then the abpove equation will produce a simple least squares fit. Thus, the training error will always (iii.) steadily decrease. (ISL,221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. As we increase s from 0, the testing RSS will typically:\n",
    "\n",
    "Increasing $s$ to a sufficiently large value, then the abpove equation will produce a simple least squares fit. Thus, the testing error will  (v.) Decrease initially, and then eventually start increasing in a U shape, as overfitting takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. As we increase s from 0, the squared bias will typically:\n",
    "\n",
    "Increasing $s$ will increase the constraint value on the possible values of $\\beta_j$. also increases. This causes the bais squared value to also (ii.) steadily increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. As we increase s from 0, the variance will typically:\n",
    "\n",
    "Increasing $s$ will generally cause the variance to (ii.) steadily increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. As we increase s from 0, the Bayes Error will typically:\n",
    "\n",
    "Increasing $s$ will cause the Bayes Error to reduce initially,, because the irreducible error always seeks the minimum possible value. However, when overfitting is comes into play, we can then say that the error will (v.) Decrease initially, and then eventually start increasing in a U shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
