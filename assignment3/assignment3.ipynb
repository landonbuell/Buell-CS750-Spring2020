{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landon Buell\n",
    "Marek Petrik\n",
    "CS 750.01\n",
    "11 Feb 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [25%] \n",
    "### Suppose that collected data for a group of machine learning students from last year. For each student,I have a feature:\n",
    "X1 = hours studied for the class every week,\n",
    "X2 = overall GPA\n",
    "Y = whether the student receives an A. \n",
    "### We ﬁt a logistic regression model and produce estimated coeﬃcients\n",
    "$\\hat{\\beta}_0 = -6 $ , $\\hat{\\beta}_1 = -0.1$ and $\\hat{\\beta}_2 = 1.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Estimate the probability of getting an A for a student who studies for 40h and has an undergrad GPA of 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of getting an A: 0.03353501304664781 % chance\n"
     ]
    }
   ],
   "source": [
    "b0,b1,b2 = -6.0,-0.1,+1.0 \n",
    "X1,X2 = 40,2.0\n",
    "\n",
    "def logistic_regression (beta0,beta1,beta2,X1,X2):\n",
    "    \"\"\" Compute  output given input & polynomial coefficients \"\"\"\n",
    "    exp = np.exp(beta0 + (beta1*X1) + (beta2*X2))\n",
    "    return exp/(1+exp)\n",
    "\n",
    "probA = logistic_regression(b0,b1,b2,X1,X2)\n",
    "print(\"Probability of getting an A:\",probA*100,\"% chance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. By how much would the student in part 1 need to improve their GPA or adjust time studied to have a 90% chance of getting an A in the class? Is that likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of getting an A: 90.02495108803149 % chance\n",
      "A student would have to improve their GPS from a 2.0 to a ~12.2 to have a 90% chance of getting an A\n"
     ]
    }
   ],
   "source": [
    "probA = logistic_regression(b0,b1,b2,X1,12.2)\n",
    "print(\"Probability of getting an A:\",probA*100,\"% chance\")\n",
    "print(\"A student would have to improve their GPS from a 2.0 to a ~12.2 to have a 90% chance of getting an A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [25%] \n",
    "### Consider a classiﬁcation problem with two classes T (true) and F (false). Then, suppose that you have the following four prediction models: \n",
    "\n",
    "#### •T: The classiﬁer predicts T for each instance (always) \n",
    "#### • F: The classiﬁer predicts F for each instance (always) \n",
    "#### • C: The classiﬁer predicts the correct label always (100% accuracy) \n",
    "#### • W: The classiﬁer predicts the wrong label always (0% accuracy) \n",
    "\n",
    "### You also have a test set with 60% instances labeled T and 40%instances labeled F. Now, compute the following statistics for each one of your algorithms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# create fake data sets\n",
    "targets = np.array([1,1,1,1,1,1,0,0,0,0])\n",
    "Cls_T = np.ones((1,10),dtype=int).transpose()\n",
    "Cls_F = np.zeros((1,10),dtype=int).transpose()\n",
    "Cls_C = np.copy(targets)\n",
    "Cls_W = np.array([0,0,0,0,0,0,1,1,1,1])\n",
    "\n",
    "# Compute vals & assemble into frame\n",
    "models = [Cls_T,Cls_F,Cls_C,Cls_W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cls. T</th>\n",
       "      <th>Cls. F</th>\n",
       "      <th>Cls. C</th>\n",
       "      <th>Cls. W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cls. T  Cls. F  Cls. C  Cls. W\n",
       "0       1       0       1       0\n",
       "1       6       0       6       0\n",
       "2       4       0       0       4\n",
       "3       0       4       4       0\n",
       "4       0       6       0       6\n",
       "5       0       0       1       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute recall scores w/ sklearn\n",
    "recalls = np.array([metrics.recall_score(targets,X) for X in models])\n",
    "truepos = np.array([metrics.confusion_matrix(targets,X)[1][1] for X in models])\n",
    "falsepos = np.array([metrics.confusion_matrix(targets,X)[0][1] for X in models])\n",
    "trueneg = np.array([metrics.confusion_matrix(targets,X)[0][0] for X in models])\n",
    "specificity = np.array([metrics.confusion_matrix(targets,X)[1][0] for X in models])\n",
    "precisions = np.array([metrics.precision_score(targets,X) for X in models])\n",
    "\n",
    "data = np.array([recalls,truepos,falsepos,trueneg,specificity,precisions])\n",
    "frame = pd.DataFrame(data=data,columns=['Cls. T','Cls. F','Cls. C','Cls. W'],dtype=int)\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [25%] \n",
    "### In this problem, you will derive the bias-variance decomposition of MSE as described in Eq. (2.7) in ISL. Let $f$ be the true model, $\\hat{f}$ be the estimated model. Consider ﬁxed instance $x_0$ with the label $y_0 = f(x_0)$. For simplicity, assume that Var[$\\epsilon$]=0, in which case the decomposition becomes:\n",
    "\n",
    "$$ E\\Big[ (y_0 - \\hat{f}(x_0)) \\Big] = Var\\Big[\\hat{f}(x_0)\\Big] + \\Bigg( E\\Big[ f(x_0) - \\hat{f}(x_0) \\Big] \\Bigg)^2 $$\n",
    "\n",
    "### Prove that the equality holds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [25%] \n",
    "### I wrote the following code that computes the MSE, bias, and variance for a test point.\n",
    "\n",
    "The problem lies in the 2nd to last line of the program. The sample variance of a dataset is given by:\n",
    "$$ \\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\n",
    "However, as per the R documentation pages, R computes the variance of a datatset as:\n",
    "$$ \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\n",
    "Which is actaully the population variance of a data set and produces a different value. So, the relationship between MSE, bias, and variance no longer holds as computed by R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
