{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landon Buell\n",
    "Marek Petrik\n",
    "CS 750.01\n",
    "5 Feb 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [30%]\n",
    "### This problem examines the use and assumptions of Linear Discrimiant Analysis (LDA) and Quadratic Discrimiant Analysis (QDA). We will be using the dataset Default from ISLR. \n",
    "\n",
    "I loaded the datset into R, and the rewrote it as a CSV to use in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0      0.0      0.0   729.526495  44361.625074\n",
       "1      0.0      1.0   817.180407  12106.134700\n",
       "2      0.0      0.0  1073.549164  31767.138947\n",
       "3      0.0      0.0   529.250605  35704.493935\n",
       "4      0.0      0.0   785.655883  38463.495879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in data from CSV and make all numeric\n",
    "default = pd.read_csv(\"Default.txt\",sep=\",\",header=0,usecols=[1,2,3,4])\n",
    "\n",
    "default = default.replace(\"No\",0)\n",
    "default = default.replace(\"Yes\",1)\n",
    "default = default.astype(float)\n",
    "\n",
    "# print head of dataframe\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split the data into a training set (70%) and a test set (30%). Then compare the classiﬁcation error of LDA, QDA, and logistic regression when predicting default as a function of features of your choice. Which method appears to work best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Disc. Analysis Classification Error: 0.971\n",
      "Quadratic Disc. Analysis Classification Error: 0.972\n",
      "Logistic Regression Classification Error: 0.9653333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data set into train & test\n",
    "y = default['default'].astype(int)\n",
    "X = default[['student','balance','income']]\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "# Create linear disc. Classifier instance\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "LDA_CLF = LDA()\n",
    "LDA_CLF.fit(xtrain,ytrain)\n",
    "LDA_pred = LDA_CLF.predict(xtest)\n",
    "\n",
    "# Create Quadratic Disc. Classifier instance\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "QDA_CLF = QDA()\n",
    "QDA_CLF.fit(xtrain,ytrain)\n",
    "QDA_pred = QDA_CLF.predict(xtest)\n",
    "\n",
    "# Create Logstic Regression \n",
    "from sklearn.linear_model import LogisticRegression as LRC\n",
    "LRC_CLF = LRC()\n",
    "LRC_CLF.fit(xtrain,ytrain)\n",
    "LRC_pred = LRC_CLF.predict(xtest)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "LDA_score = accuracy_score(ytest,LDA_pred)\n",
    "QDA_score = accuracy_score(ytest,QDA_pred)\n",
    "LRC_score = accuracy_score(ytest,LRC_pred)\n",
    "\n",
    "\n",
    "print(\"Linear Disc. Analysis Classification Error:\",LDA_score)\n",
    "print(\"Quadratic Disc. Analysis Classification Error:\",QDA_score)\n",
    "print(\"Logistic Regression Classification Error:\",LRC_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three models produce very similar resuts for this particular problem. However, I would favor the QDA model due to it's general flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Report the confusion table for each classiﬁcation method. Make sure to label which dimension is the predicted class and which one is the true class. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Disc. Analysis Confusion Matrix:\n",
      "[[2888   11]\n",
      " [  76   25]]\n",
      "Quadratic Disc. Analysis Confusion Matrix:\n",
      "[[2885   14]\n",
      " [  70   31]]\n",
      "Logisitc Regression Confusion Matrix:\n",
      "[[2896    3]\n",
      " [ 101    0]]\n"
     ]
    }
   ],
   "source": [
    "# build 3 confusion matricies\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Linear Disc. Analysis Confusion Matrix:\")\n",
    "LDA_confmat = confusion_matrix(ytest,LDA_pred)\n",
    "print(LDA_confmat)\n",
    "\n",
    "print(\"Quadratic Disc. Analysis Confusion Matrix:\")\n",
    "QDA_confmat = confusion_matrix(ytest,QDA_pred)\n",
    "print(QDA_confmat)\n",
    "\n",
    "print(\"Logisitc Regression Confusion Matrix:\")\n",
    "LRC_confmat = confusion_matrix(ytest,LRC_pred)\n",
    "print(LRC_confmat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each confusion matrix has a string entry at index (1,1) - the True Positive entry. This means that It is excellent at correctly identifying those who belong into class '0': i.e. those who will NOT default on their loans. However, entry (2,2) seems to be very weak, this means that each method is poor at identifying any instance in class \"1\": those who did default on their loans. Furthermore, the entry (2,1) is generally much stronger, indicating that that model predicted the person would not default, even though they did. We usually want a confusion matrix with a strong main diagonal and weaker off-diagonals. These matricies do not grant us that so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Are the LDA assumptions satisﬁed when predicting default as a function of balance only (i.e default ~ balance)? You can use qqnorm and qqline to examine whether the conditional class distributions are normally distributed. Also examine standard deviations of the class distributions. Are the QDA assumptions satisﬁed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Would you ever want to use LDA in place of QDA even when you suspect that some of the assumptions are violated (e.g. diﬀerent conditional standard deviations) for LDA? Hint: Check out TidyVerse for a collection of packages that can help with data manipulation. And see the Rstudio cheatsheets for a convenient and concise reference to the methods. This is entirely optional!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [30%] \n",
    "### Using the MNIST dataset, ﬁt classiﬁcation models in order to predict the digit 1 (vs all others).\n",
    "\n",
    "#### 1. Compare the classiﬁcation error for each one of these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load in MNIST data set\n",
    "mnist = fetch_openml('mnist_784',version=1)\n",
    "print(mnist.keys())\n",
    "X,y = mnist['data'],mnist['target'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train-test sets\n",
    "xtrain,xtest,ytrain,ytest = X[:60000],X[60000:],y[:60000],y[60000:]\n",
    "# isolate 1's in targets set\n",
    "ytrain1,ytest1 = (ytrain == 1),(ytest == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create & train LR model\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(xtrain,ytrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model & build confusion matrix\n",
    "LR_predict = LR_model.predict(xtest)\n",
    "LR_confmat = confusion_matrix(ytest1,LR_predict)\n",
    "# print matrix\n",
    "print(\"Confusion Matrix for Logisitic Regression:\")\n",
    "print(LR_confmat)\n",
    "print(\"\\nClassification Report:\\n\",classification_report(ytest1,LR_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 10,000 samples tested, the logisitic regression classifier correctly classifier ~8800 of them as 1's and ~ 1100 of them as not 1's. This is shown in the confusion matrx above, notice it's strng diagonal entries. By comparison, the off-diagonals have quite small enties. Additionally, the precision, recall and F1-Scores of this classifier are all quite strong. The model seems to do a good jonb a finding all of the 1's and extracting all of the not 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. K-NN (with 2 reasonable choices of k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "\n",
    "# KNN Classifier for K = 10\n",
    "K = 10\n",
    "KNN_Clf = KNC(n_neighbors=K)\n",
    "KNN_Clf.fit(xtrain,ytrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model & build confusion matrix for K = 10\n",
    "KNN_pred = KNN_Clf.predict(xtest)\n",
    "KNN_confmat = confusion_matrix(ytest1,KNN_pred)\n",
    "# print matrix\n",
    "print(\"Confusion Matrix for 10-Nearest Neighbors Classifier:\")\n",
    "print(KNN_confmat)\n",
    "print(\"\\nClassification Report:\\n\",classification_report(ytest1,KNN_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier for K = 10\n",
    "K = 5\n",
    "KNN_Clf = KNC(n_neighbors=K)\n",
    "KNN_Clf.fit(xtrain,ytrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model & build confusion matrix for K = 50\n",
    "KNN_pred = KNN_Clf.predict(xtest)\n",
    "KNN_confmat = confusion_matrix(ytest1,KNN_pred)\n",
    "# print matrix\n",
    "print(\"Confusion Matrix for 50-Nearest Neighbors Classifier:\")\n",
    "print(KNN_confmat)\n",
    "print(\"\\nClassification Report:\\n\",classification_report(ytest1,KNN_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ytrain1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1e410a6a32ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# create and train classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLDA_Clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mLDA_Clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# test & evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ytrain1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# create and train classifier\n",
    "LDA_Clf = LDA()\n",
    "LDA_Clf.fit(xtrain,ytrain1)\n",
    "\n",
    "# test & evaluate model\n",
    "LDA_pred = LDA_Clf.predict(xtest)\n",
    "LDA_confmat = confusion_matrix(ytest1,LDA_pred)\n",
    "\n",
    "# print matrix & report\n",
    "print(\"Confusion Matrix for 50-Nearest Neighbors Classifier:\")\n",
    "print(KNN_confmat)\n",
    "print(\"\\nClassification Report:\\n\",classification_report(ytest1,LDA_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [20%]\n",
    "#### Logistic regression uses the logistic function to predict class probabilities: \n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} $$\n",
    "\n",
    "#### Which is Equivalent to the linear model for the predicition of the log-odds:\n",
    "\n",
    "$$ \\log\\Big(\\frac{p(X)}{1-p(x)}\\Big) = \\beta_0 + \\beta_1X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show equivalence by first making the variable substitution such that : $\\alpha = \\beta_0 + \\beta_1X$ to ease reading. Thus, now we can write our Log-odds function given the defintion of $p(X)$  from logisitc regression:\n",
    "\n",
    "$$ \\log \\Bigg( \\frac{\\frac{e^\\alpha}{1+e^\\alpha}}{1+\\frac{e^\\alpha}{1+e^\\alpha}} \\Bigg) $$\n",
    "\n",
    "We can then modify the denominator of the expression, and re-write $1$ as $\\frac{1+e^\\alpha}{1+e^\\alpha}$ to allow for the combination of fraction terms with a common denominator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, now our log-odds equation reads:\n",
    "    \n",
    "$$ \\log \\Bigg( \\frac{\\frac{e^\\alpha}{1+e^\\alpha}}{\\frac{1+e^\\alpha}{1+e^\\alpha}+\\frac{e^\\alpha}{1+e^\\alpha}} \\Bigg) $$\n",
    "\n",
    "To which we combine common denominators:\n",
    "\n",
    "$$ \\log \\Bigg( \\frac{\\frac{e^\\alpha}{1+e^\\alpha}}{\\frac{1+e^\\alpha-e^\\alpha}{1+e^\\alpha}} \\Bigg) $$\n",
    "\n",
    "Which in turn becomes:\n",
    "\n",
    "$$ \\log \\Bigg( \\frac{\\frac{e^\\alpha}{1+e^\\alpha}}{\\frac{1}{1+e^\\alpha}} \\Bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now rearange the argument inside the $\\log()$ operation by taking advantage of the relationship between multiplication and division:\n",
    "\n",
    "$$ \\log \\Bigg( \\Big(\\frac{e^\\alpha}{1+e^\\alpha}\\Big) \\Big( 1 + e^\\alpha \\Big) \\Bigg) $$\n",
    "\n",
    "We can then cross out the cancelling terms, and simply to arrive at:\n",
    "\n",
    "$$ \\log\\big(e^\\alpha\\big) $$\n",
    "\n",
    "Which by definition is equivalent to $\\alpha$, which we have defined to be  $\\alpha = \\beta_0 + \\beta_1X$.\n",
    "Thus The expressions are mathematically equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [20%]\n",
    "\n",
    "### This problem examines the diﬀerences between LDA and QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. For an arbitrary training set, would you expect for LDA or QDA to work better on the training set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an arbitrary training, set I would expect QDA to function a little bit better than LDA. In the real world, there are very few relations between variables that are truly linear, this a quadratic approach would seem more generallt fitting. Also, in some case of an actual linear relationship, the $\\beta_2$ coefficient would go close to zero, indiciating a linear approach is more useful for that particular case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. If the Bayes decision boundary between the two classes is linear, would you expect LDA or QDA to work better on the training set? What about the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the decision boundary is linear, then the linear model, LDA would seem to be the far more reasonable model to use. Thus it would produce a better fit on the training data set, and be a better prediction model for the testing data set as well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. As the sample size increases, do you expect the prediction accuracy of QDA with respect to LDA increase or decrease \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. True or False: Even if the Bayesian decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is more ﬂexible and can model a linear decision boundary. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False. Sometimes, if a relationship is linear, the approximation of it should be a linear model. In the case of a quadratic model, like QDA, the best best fit curve may actually produce and overfit, or be useless at the bounds of the expression. Even if QDA is more flexible, it has the potential to misbehave in these senses, cause misinformation to arise in the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
