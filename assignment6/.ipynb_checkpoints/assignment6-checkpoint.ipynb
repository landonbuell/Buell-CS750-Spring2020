{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landon Buell\n",
    "Marek Petrik\n",
    "CS 750.01\n",
    "21 March 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [33%] \n",
    "In this problem, we will establish some basic properties of vectors and linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The L2 norm of a vector measures the length (size) of a vector. The norm for a vector x of size n is deﬁned as: \n",
    "$$ ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2} $$\n",
    "#### Show that the norm can be expressed as the following quadratic expression: \n",
    "$$ ||x||_2^2 = x^T x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the definition of the $L_2$ norm above, we can compute the square of it. THis removes the radical, and then the norm, squared is just the sum of all elements in the vector $x$, squared. Thus:\n",
    "$$ ||x||_2^2 = \\sum_{i=1}^{n}x_i^2 $$\n",
    "We can compare this to the vector $x$, by itself, it is a column vector with $n$ entres, shaped $1 \\times n$. The corresponding transpose, $x^T$, then also has $n$ elements, but is a row vector containing the same elements, shaped $m \\times 1$. When computing their matrix multiplication, or in this case, dot product, $x^T x$. The result is the $i$-th element in each array multiplied by the same element in the other array, and them summed. Given that they are just mutual transposes, corresponding elemtns are identitcial so: \n",
    "$$x^T_i = x_i \\rightarrow x^T_i x_i = x_i^2 $$\n",
    "Finally, summing over all elements in the arrays just produces the result:\n",
    "$$ ||x||_2^2 = \\sum_{i=1}^{n}x_i^2 $$\n",
    "Which we have shown above to be equivalent to the square of the $L_2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Let $a$ and $x$ be vectors of size $n = 3$ and consider the following linear function $f(x)= a^T x$. Show that the gradient of $f$ is: $\\nabla_x f(x) = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating $f(x) = a^T x$, we see that it becomes the dot product of vectors $a$ and $x$:\n",
    "$$a^T x = a_1 x_1 + a_2 x_2 + a_3 x_3 $$\n",
    "So, if we take the gradient of $f(x)$, with respect to each elment in the $x$ vector:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \n",
    "\\end{bmatrix} $$\n",
    "Using rules of partial differntiation from calculus 3, we can then evaluate this gradient to be:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\ a_2 \\\\ a_3 \n",
    "\\end{bmatrix}$$\n",
    "Which is indentically the vector $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Let $A$ be a symmetric matrix of size $3 \\times 3$ and consider the following quadratic function $f(x) = x^T Ax$. Show that the gradient of $f$ is: $\\nabla_x f(x) = 2Ax$. A matrix is symmetric if $A_{ij} = A_{ji}$ for all $i$ and $j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluatiing out $A\\vec{x}$ is:\n",
    "$$ Ax =\n",
    "\\begin{bmatrix}\n",
    "A_{11} A_{12} A_{13} \\\\\n",
    "A_{21} A_{22} A_{23} \\\\\n",
    "A_{31} A_{32} A_{33}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "A_{11}x_1 + A_{12}x_2 + A_{13}x_3 , \\\\\n",
    "A_{21}x_1 + A_{22}x_2 + A_{23}x_3 , \\\\\n",
    "A_{31}x_1 + A_{32}x_2 + A_{33}x_3\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can then left multiply by $x^T$:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(x_1(A_{11}x_1 + A_{12}x_2 + A_{13}x_3)\\Big) +\n",
    "\\Big(x_2(A_{21}x_1 + A_{22}x_2 + A_{13}x_3)\\Big) +\n",
    "\\Big(x_3(A_{31}x_1 + A_{32}x_2 + A_{23}x_3)\\Big) \n",
    "$$\n",
    "Which also:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(A_{11}x_1^2 + A_{12}x_2 x_1 + A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{21}x_1 x_2 + A_{22}x_2^2 + A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{31}x_1 x_3 + A_{32}x_2 x_3 + A_{33}x_3^2 \\Big) \n",
    "$$\n",
    "We can exploit the symmetry of the matrix $A$, and replace all elements below the main diagonal with the corresponding elements above the main diagonal:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(A_{11}x_1^2 + A_{12}x_2 x_1 + A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{12}x_1 x_2 + A_{22}x_2^2 + A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{13}x_1 x_3 + A_{23}x_2 x_3 + A_{33}x_3^2 \\Big) \n",
    "$$\n",
    "Combine like-terms:\n",
    "$$ f(x) = x^T (Ax) = \n",
    "\\Big(A_{11}x_1^2 + 2A_{12}x_2 x_1 + 2A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{22}x_2^2 + 2A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{33}x_3^2 \\Big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the gradient of $f(x)$ as defined above. This, along with the chain rule of calculus:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2A_{11}x_1 + 2A_{12}x_2 + 2A_{13}x_3 \\\\\n",
    "2A_{12}x_1 + 2A_{22}x_2 + 2A_{23}x_3 \\\\\n",
    "2A_{13}x_1 + 2A_{23}x_2 + 2A_{33}x_3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Once again, exploit the symmetry of the matrix to restore incidies, and then remove the common factor of $2$ from all matrix elements, and then decpmosing the system back into a matrix-vector equation:\n",
    "$$ \\nabla_x\\big[f(x)\\big] = 2\n",
    "\\begin{bmatrix}\n",
    "A_{11} A_{12} A_{13} \\\\\n",
    "A_{21} A_{22} A_{23} \\\\\n",
    "A_{31} A_{32} A_{33}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}  $$\n",
    "Which is mathematically equaivalent to $2Ax$, thus $\\nabla_x\\big[f(x)\\big] = \\nabla_x\\big[x^T Ax\\big] = 2Ax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [34%] \n",
    "### Hint: You can follow the slides from the March 4th class, or the LAR reference from the class website. See the class website for some recommended linear algebra references. \n",
    "You will derive the formula used to compute the solution to ridge regression. The objective in ridge regression is: \n",
    "$$ f(\\beta) = ||y - A\\beta||_2^2 +\\lambda||\\beta||_2^2 $$\n",
    "Here, $\\beta$ is the vector of coeﬃcients that we want to optimize, $A$ is the design matrix, $y$ is the target, and $\\lambda$is the regularization coefficient. The notation $||\\cdot||_2$ represents the Euclidean (or $L_2$) norm.\n",
    "Our goal is to find $\\beta$ that solves:\n",
    "$$ \\min_{\\beta}f(\\beta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Express the ridge regression objective $f(\\beta)$ in terms of linear and quadratic terms. Recall that $||\\beta||_2^2 = \\beta^T \\beta$.The result should be similar to the objective function of linear regression.\n",
    "\n",
    "Expand $f(\\beta)$ as a polynomial:\n",
    "$$ f(\\beta) = \\big(y - A\\beta\\big)^T \\big(y - A\\beta\\big) + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "Then 'distribute' out the transposition:\n",
    "$$ f(\\beta) = \\big(y^T - \\beta^TA^T\\big)\\big(y - A\\beta\\big) + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "Evaluate ('foil') out to build the polynomial:\n",
    "$$ f(\\beta) = y^Ty - y^TA\\beta -\\beta^TA^Ty + \\beta^TA^TA\\beta + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "And then simplify, using rules of transposition:\n",
    "$$ f(\\beta) = y^Ty - 2y^TA\\beta + \\beta^TA^TA\\beta + \\lambda\\big(\\beta^T \\beta\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Derive the gradient: $\\nabla_\\beta\\Big[f(\\beta)\\Big]$ using the linear and quadratic terms above.\n",
    "The value of $\\nabla_\\beta\\Big[f(\\beta)\\Big]$ is:\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\frac{df}{d\\beta}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-2y^TA + \\beta^TA^TA + \\lambda\\beta^T \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Since $f$ is convex, its minimal value is attained when :\n",
    "$$ \\nabla_\\beta\\Big[f(\\beta)\\Big] = 0 $$\n",
    "#### Derive the expression for $\\beta$ that satisifies the inequality above.\n",
    "We can transpose the gradient of the function $f$, and equate it to $0^T$, which is still $0$:\n",
    "$$ -2A^Ty + 2A^TA\\beta + \\lambda\\beta = 0 $$\n",
    "Thus $f(\\beta) = 0$ when:\n",
    "$$ \\beta = \\frac{2A^T y}{2A^TA+\\lambda} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Implement the algorithm for computing $\\beta$ and use it on a small dataset of your choice. Do not forget about the intercept.\n",
    "\n",
    "let $\\lambda = 1$. Let $A$ be the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [33%] \n",
    "### Using the MNIST dataset, which we used already in Assignment 2, compare whether boosting, bagging, and random forests work the best. You may may want to use only a subset of the data. Optional: Use xgboost (also available for Python) to see whether the results are better than other boosting methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n",
      "(5000, 784) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Load in MNIST data\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X,y = fetch_openml('mnist_784',version=1,return_X_y=True)\n",
    "X_train,y_train = X[:10000],y[:10000]\n",
    "X_test,y_test = X[10000:15000],y[10000:15000]\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       495\n",
      "           1       0.95      0.98      0.97       563\n",
      "           2       0.88      0.90      0.89       471\n",
      "           3       0.92      0.86      0.89       516\n",
      "           4       0.90      0.93      0.91       488\n",
      "           5       0.91      0.91      0.91       455\n",
      "           6       0.93      0.93      0.93       476\n",
      "           7       0.94      0.94      0.94       523\n",
      "           8       0.88      0.88      0.88       488\n",
      "           9       0.90      0.89      0.90       525\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.92      0.92      0.92      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sklearn Boosting \"\"\" \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "GB_CLF = GradientBoostingClassifier()\n",
    "GB_CLF.fit(X_train,y_train)\n",
    "\n",
    "y_pred = GB_CLF.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test,y_pred,labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       495\n",
      "           1       0.95      0.98      0.97       563\n",
      "           2       0.84      0.87      0.85       471\n",
      "           3       0.85      0.84      0.84       516\n",
      "           4       0.89      0.90      0.89       488\n",
      "           5       0.89      0.86      0.88       455\n",
      "           6       0.93      0.90      0.91       476\n",
      "           7       0.94      0.92      0.93       523\n",
      "           8       0.84      0.83      0.84       488\n",
      "           9       0.87      0.84      0.86       525\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sklearn Bagging \"\"\"\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "Bag_CLF = BaggingClassifier()\n",
    "Bag_CLF.fit(X_train,y_train)\n",
    "\n",
    "y_pred = Bag_CLF.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test,y_pred,labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       495\n",
      "           1       0.94      0.98      0.96       563\n",
      "           2       0.84      0.90      0.87       471\n",
      "           3       0.83      0.85      0.84       516\n",
      "           4       0.87      0.90      0.88       488\n",
      "           5       0.91      0.87      0.89       455\n",
      "           6       0.95      0.91      0.93       476\n",
      "           7       0.93      0.93      0.93       523\n",
      "           8       0.88      0.80      0.84       488\n",
      "           9       0.89      0.85      0.87       525\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Landon\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sklearn Random Forests \"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_CLF = RandomForestClassifier()\n",
    "RF_CLF.fit(X_train,y_train)\n",
    "\n",
    "y_pred = RF_CLF.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test,y_pred,labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In running the three models form the sklearn.ensembele sub-module, I took the precision, recall and F1 scores for each method. The results after running each model a few different times shows that the Gradient Boosting Classification method seems to produce the best results. Each time, it had the highest average score for eahc class between the three metrics. Unfortunately, this method also took the logest to fit and predict a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Problem 4 [+10%] \n",
    "### Hint: We did not cover this material in class, but it is important regardless. The slides from March 9th may be helpful when implementing this. In this problem, you will implement a gradient descent algorithm for solving a linear regression problem. Recall that the RSS objective in linear regression is: \n",
    "$$ f(\\beta) = ||y - A\\beta||^2_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Consider the problem of predicting revenue as a function of spending on TV and Radio advertising. There are only 4 data points:\n",
    "$$\\begin{bmatrix}\n",
    "\\text{Revenue} & \\text{TV} & \\text{Radio} \\\\\n",
    "20 & 3 & 7 \\\\\n",
    "14 & 4 & 6 \\\\\n",
    "32 & 6 & 1 \\\\\n",
    "5 & 1 & 1 \n",
    "\\end{bmatrix} $$\n",
    "#### Write down the design matrix of predictors, $A$ , and the response vector, $y$, for this regression problem. Do not forget about the intercept, which can be modeled as a predictor with a constant value over all data points. The matrix $A$ should be $4 \\times 3$ dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
