{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landon Buell\n",
    "Marek Petrik\n",
    "CS 750.01\n",
    "21 March 2020\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [33%] \n",
    "In this problem, we will establish some basic properties of vectors and linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The L2 norm of a vector measures the length (size) of a vector. The norm for a vector x of size n is deﬁned as: \n",
    "$$ ||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2} $$\n",
    "#### Show that the norm can be expressed as the following quadratic expression: \n",
    "$$ ||x||_2^2 = x^T x $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the definition of the $L_2$ norm above, we can compute the square of it. THis removes the radical, and then the norm, squared is just the sum of all elements in the vector $x$, squared. Thus:\n",
    "$$ ||x||_2^2 = \\sum_{i=1}^{n}x_i^2 $$\n",
    "We can compare this to the vector $x$, by itself, it is a column vector with $n$ entres, shaped $1 \\times n$. The corresponding transpose, $x^T$, then also has $n$ elements, but is a row vector containing the same elements, shaped $m \\times 1$. When computing their matrix multiplication, or in this case, dot product, $x^T x$. The result is the $i$-th element in each array multiplied by the same element in the other array, and them summed. Given that they are just mutual transposes, corresponding elemtns are identitcial so: \n",
    "$$x^T_i = x_i \\rightarrow x^T_i x_i = x_i^2 $$\n",
    "Finally, summing over all elements in the arrays just produces the result:\n",
    "$$ ||x||_2^2 = \\sum_{i=1}^{n}x_i^2 $$\n",
    "Which we have shown above to be equivalent to the square of the $L_2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Let $a$ and $x$ be vectors of size $n = 3$ and consider the following linear function $f(x)= a^T x$. Show that the gradient of $f$ is: $\\nabla_x f(x) = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating $f(x) = a^T x$, we see that it becomes the dot product of vectors $a$ and $x$:\n",
    "$$a^T x = a_1 x_1 + a_2 x_2 + a_3 x_3 $$\n",
    "So, if we take the gradient of $f(x)$, with respect to each elment in the $x$ vector:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \n",
    "\\end{bmatrix} $$\n",
    "Using rules of partial differntiation from calculus 3, we can then evaluate this gradient to be:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\ a_2 \\\\ a_3 \n",
    "\\end{bmatrix}$$\n",
    "Which is indentically the vector $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Let $A$ be a symmetric matrix of size $3 \\times 3$ and consider the following quadratic function $f(x) = x^T Ax$. Show that the gradient of $f$ is: $\\nabla_x f(x) = 2Ax$. A matrix is symmetric if $A_{ij} = A_{ji}$ for all $i$ and $j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluatiing out $A\\vec{x}$ is:\n",
    "$$ Ax =\n",
    "\\begin{bmatrix}\n",
    "A_{11} A_{12} A_{13} \\\\\n",
    "A_{21} A_{22} A_{23} \\\\\n",
    "A_{31} A_{32} A_{33}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "A_{11}x_1 + A_{12}x_2 + A_{13}x_3 , \\\\\n",
    "A_{21}x_1 + A_{22}x_2 + A_{23}x_3 , \\\\\n",
    "A_{31}x_1 + A_{32}x_2 + A_{33}x_3\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can then left multiply by $x^T$:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(x_1(A_{11}x_1 + A_{12}x_2 + A_{13}x_3)\\Big) +\n",
    "\\Big(x_2(A_{21}x_1 + A_{22}x_2 + A_{13}x_3)\\Big) +\n",
    "\\Big(x_3(A_{31}x_1 + A_{32}x_2 + A_{23}x_3)\\Big) \n",
    "$$\n",
    "Which also:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(A_{11}x_1^2 + A_{12}x_2 x_1 + A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{21}x_1 x_2 + A_{22}x_2^2 + A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{31}x_1 x_3 + A_{32}x_2 x_3 + A_{33}x_3^2 \\Big) \n",
    "$$\n",
    "We can exploit the symmetry of the matrix $A$, and replace all elements below the main diagonal with the corresponding elements above the main diagonal:\n",
    "$$ x^T(Ax) = \n",
    "\\Big(A_{11}x_1^2 + A_{12}x_2 x_1 + A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{12}x_1 x_2 + A_{22}x_2^2 + A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{13}x_1 x_3 + A_{23}x_2 x_3 + A_{33}x_3^2 \\Big) \n",
    "$$\n",
    "Combine like-terms:\n",
    "$$ f(x) = x^T (Ax) = \n",
    "\\Big(A_{11}x_1^2 + 2A_{12}x_2 x_1 + 2A_{13}x_3 x_1 \\Big) +\n",
    "\\Big(A_{22}x_2^2 + 2A_{23}x_3 x_2 \\Big) +\n",
    "\\Big(A_{33}x_3^2 \\Big) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then take the gradient of $f(x)$ as defined above. This, along with the chain rule of calculus:\n",
    "$$\\nabla_x f(x) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}  \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2A_{11}x_1 + 2A_{12}x_2 + 2A_{13}x_3 \\\\\n",
    "2A_{12}x_1 + 2A_{22}x_2 + 2A_{23}x_3 \\\\\n",
    "2A_{13}x_1 + 2A_{23}x_2 + 2A_{33}x_3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Once again, exploit the symmetry of the matrix to restore incidies, and then remove the common factor of $2$ from all matrix elements, and then decpmosing the system back into a matrix-vector equation:\n",
    "$$ \\nabla_x\\big[f(x)\\big] = 2\n",
    "\\begin{bmatrix}\n",
    "A_{11} A_{12} A_{13} \\\\\n",
    "A_{21} A_{22} A_{23} \\\\\n",
    "A_{31} A_{32} A_{33}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}  $$\n",
    "Which is mathematically equaivalent to $2Ax$, thus $\\nabla_x\\big[f(x)\\big] = \\nabla_x\\big[x^T Ax\\big] = 2Ax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [34%] \n",
    "### Hint: You can follow the slides from the March 4th class, or the LAR reference from the class website. See the class website for some recommended linear algebra references. \n",
    "You will derive the formula used to compute the solution to ridge regression. The objective in ridge regression is: \n",
    "$$ f(\\beta) = ||y - A\\beta||_2^2 +\\lambda||\\beta||_2^2 $$\n",
    "Here, $\\beta$ is the vector of coeﬃcients that we want to optimize, $A$ is the design matrix, $y$ is the target, and $\\lambda$is the regularization coefficient. The notation $||\\cdot||_2$ represents the Euclidean (or $L_2$) norm.\n",
    "Our goal is to find $\\beta$ that solves:\n",
    "$$ \\min_{\\beta}f(\\beta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Express the ridge regression objective $f(\\beta)$ in terms of linear and quadratic terms. Recall that $||\\beta||_2^2 = \\beta^T \\beta$.The result should be similar to the objective function of linear regression.\n",
    "\n",
    "Expand $f(\\beta)$ as a polynomial:\n",
    "$$ f(\\beta) = \\big(y - A\\beta\\big)^T \\big(y - A\\beta\\big) + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "Then 'distribute' out the transposition:\n",
    "$$ f(\\beta) = \\big(y^T - \\beta^TA^T\\big)\\big(y - A\\beta\\big) + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "Evaluate ('foil') out to build the polynomial:\n",
    "$$ f(\\beta) = y^Ty - y^TA\\beta -\\beta^TA^Ty + \\beta^TA^TA\\beta + \\lambda\\big(\\beta^T \\beta\\big) $$\n",
    "And then simplify, using rules of transposition:\n",
    "$$ f(\\beta) = y^Ty - 2y^TA\\beta + \\beta^TA^TA\\beta + \\lambda\\big(\\beta^T \\beta\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Derive the gradient: $\\nabla_\\beta\\Big[f(\\beta)\\Big]$ using the linear and quadratic terms above.\n",
    "The value of $\\nabla_\\beta\\Big[f(\\beta)\\Big]$ is:\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\frac{df}{d\\beta}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-2y^TA + \\beta^TA^TA + \\lambda\\beta^T \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Since $f$ is convex, its minimal value is attained when :\n",
    "$$ \\nabla_\\beta\\Big[f(\\beta)\\Big] = 0 $$\n",
    "#### Derive the expression for $\\beta$ that satisifies the inequality above.\n",
    "We can transpose the gradient of the function $f$, and equate it to $0^T$, which is still $0$:\n",
    "$$ -2A^Ty + 2A^TA\\beta + \\lambda\\beta = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 [33%] \n",
    "### Using the MNIST dataset, which we used already in Assignment 2, compare whether boosting, bagging, and random forests work the best. You may may want to use only a subset of the data. Optional: Use xgboost (also available for Python) to see whether the results are better than other boosting methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Problem 4 [+10%] \n",
    "### Hint: We did not cover this material in class, but it is important regardless. The slides from March 9th may be helpful when implementing this. In this problem, you will implement a gradient descent algorithm for solving a linear regression problem. Recall that the RSS objective in linear regression is: \n",
    "$$ f(\\beta) = ||y - A\\beta||^2_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Consider the problem of predicting revenue as a function of spending on TV and Radio advertising. There are only 4 data points:\n",
    "$$\\begin{bmatrix}\n",
    "\\text{Revenue} & \\text{TV} & \\text{Radio} \\\\\n",
    "20 & 3 & 7 \\\\\n",
    "14 & 4 & 6 \\\\\n",
    "32 & 6 & 1 \\\\\n",
    "5 & 1 & 1 \n",
    "\\end{bmatrix} $$\n",
    "#### Write down the design matrix of predictors, $A$ , and the response vector, $y$, for this regression problem. Do not forget about the intercept, which can be modeled as a predictor with a constant value over all data points. The matrix $A$ should be $4 \\times 3$ dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
